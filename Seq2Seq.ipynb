{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOm6ziPy8xcjNmOeR5sIypn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cj12o/Pytorch/blob/main/Seq2Seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "KXWQgsDxCTne"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self,input_dim,emb_dim,hidden_dim,num_layers=1):\n",
        "    super(Encoder,self).__init__()\n",
        "    self.embedding=nn.Embedding(input_dim,emb_dim)\n",
        "    self.gru=nn.GRU(emb_dim,hidden_dim,num_layers,batch_first=True)\n",
        "  def forward(self,src):\n",
        "    l1_op=self.embedding(src)\n",
        "    output,hidden=self.gru(l1_op)\n",
        "    return hidden\n",
        "#note hidden_dim->1) hyper param for Gru,it represents no:of gfeatures in hidden layes\n",
        "                  #2)it basically represent gru capacity to learm and represent seq. data\n",
        "# A larger hidden_dim means the hidden state can store more information, potentially allowing the network to\n",
        "# capture more intricate relationships in the sequence. However, a larger hidden_dim also increases the number of\n",
        "# parameters in the model, which can lead to increased computational cost\n",
        "#  and potentially overfitting if the dataset is not large enough."
      ],
      "metadata": {
        "id": "O7DEdHY6X6vZ"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Tp6RiB-hfudX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self,output_dim,emb_dim,hidden_dim,num_layers=1):\n",
        "    super(Decoder,self).__init__()\n",
        "    self.embeddings=nn.embedding=nn.Embedding(output_dim,emb_dim)\n",
        "    self.gru=nn.GRU(emb_dim,hidden_dim,num_layers,batch_first=True)\n",
        "    self.fc_out=nn.Linear(hidden_dim,output_dim)\n",
        "  def forward(self,input,hidden):\n",
        "    input=input.unsqueeze(1)  # [batch, 1]\n",
        "    l1_op=self.embeddings(input)  #[batch,emb_dim]\n",
        "    output,hidden=self.gru(l1_op)\n",
        "    l3_op=self.fc_out(output.squeeze(1)) #[batch,output_dim]\n",
        "    prediction = self.fc_out(output.squeeze(1))  # [batch, output_dim]\n",
        "    return hidden,prediction\n"
      ],
      "metadata": {
        "id": "B1KdKUprcS4o"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2seq(nn.Module):\n",
        "  def __init__(self,encoder,decoder,device):\n",
        "    super(Seq2seq,self).__init__()\n",
        "    self.encoder=encoder\n",
        "    self.decoder=decoder\n",
        "    self.device=device\n",
        "  def forward(self,src,target,teacher_forcing_ratio=0.5):\n",
        "    batch_size=src.size(0)\n",
        "    target_len = target.size(1)\n",
        "    target_vocab_size = self.decoder.fc_out.out_features\n",
        "    outputs = torch.zeros(batch_size,target_len,target_vocab_size).to(self.device)\n",
        "    hidden=self.encoder(src)\n",
        "    input=target[:,0] #<Start>\n",
        "    for t in range(1, target_len):\n",
        "            hidden,prediction = self.decoder(input, hidden)\n",
        "            outputs[:, t] = output\n",
        "            top1 = output.argmax(1)\n",
        "            input = target[:, t] if torch.rand(1).item() < teacher_forcing_ratio else top1\n",
        "\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "zLTNVLzniCXh"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2seq(nn.Module):\n",
        "  def __init__(self,encoder,decoder,device):\n",
        "    super(Seq2seq,self).__init__()\n",
        "    self.encoder=encoder\n",
        "    self.decoder=decoder\n",
        "    self.device=device\n",
        "  def forward(self,src,target,teacher_forcing_ratio=0.5):\n",
        "    batch_size=src.size(0)\n",
        "    target_len = target.size(1)\n",
        "    target_vocab_size = self.decoder.fc_out.out_features\n",
        "    outputs = torch.zeros(batch_size,target_len,target_vocab_size).to(self.device)\n",
        "    hidden=self.encoder(src)\n",
        "    input=target[:,0] #<Start> token - assuming target starts with a start token\n",
        "    for t in range(1, target_len):\n",
        "            # The decoder returns (hidden_state, prediction)\n",
        "            # Correctly assign the returned values to variables\n",
        "            decoder_hidden, prediction = self.decoder(input, hidden)\n",
        "\n",
        "            # The next hidden state for the decoder is the one just returned\n",
        "            hidden = decoder_hidden\n",
        "\n",
        "            # Store the prediction (output) in the outputs tensor\n",
        "            outputs[:, t] = prediction # Assign prediction (shape [batch, vocab_size]) to outputs[:, t] (shape [batch, vocab_size])\n",
        "\n",
        "            # Decide the next input based on teacher forcing\n",
        "            top1 = prediction.argmax(1) # Get the predicted next token\n",
        "            input = target[:, t] if torch.rand(1).item() < teacher_forcing_ratio else top1\n",
        "\n",
        "    # Note: The outputs[:, 0] is still zeros as we started the loop from t=1.\n",
        "    # If your target sequence includes a start token at index 0, and you want\n",
        "    # the prediction for the token at index 1 to be stored at outputs[:, 1],\n",
        "    # this indexing is correct. If you want to store the prediction for index 0\n",
        "    # (which would be based on some initial input and hidden state before the loop),\n",
        "    # you would need to handle that separately or adjust the loop range and indexing.\n",
        "    # Assuming the first prediction corresponds to the token at target index 1.\n",
        "\n",
        "    return outputs\n",
        "\n",
        "# %%\n",
        "# The subsequent code remains the same\n",
        "INPUT_DIM = 1000   # Vocabulary size of source\n",
        "OUTPUT_DIM = 1000  # Vocabulary size of target\n",
        "EMB_DIM = 256\n",
        "HIDDEN_DIM = 512\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(DEVICE)\n",
        "\n",
        "enc = Encoder(INPUT_DIM, EMB_DIM, HIDDEN_DIM).to(DEVICE)\n",
        "dec = Decoder(OUTPUT_DIM, EMB_DIM, HIDDEN_DIM).to(DEVICE)\n",
        "model = Seq2seq(enc, dec, DEVICE)\n",
        "model=model.to(DEVICE)\n",
        "\n",
        "# Sample batch: (batch_size=32, sequence_length=10)\n",
        "src = torch.randint(0, INPUT_DIM, (32, 10)).to(DEVICE)\n",
        "trg = torch.randint(0, OUTPUT_DIM, (32, 12)).to(DEVICE)\n",
        "\n",
        "output = model(src, trg)\n",
        "print(output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7Z19d5gpMdL",
        "outputId": "e07b9335-943a-4edf-a883-cef56498c372"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n",
            "torch.Size([32, 12, 1000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e9yywn12pv9v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}